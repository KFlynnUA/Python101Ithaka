{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "This notebook was created by [William Mattingly](https://datascience.si.edu/people/dr-william-mattingly) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "The exercises in this notebook are based on the notebooks created by [Zoe LeBlanc](https://ischool.illinois.edu/people/zoe-leblanc) for the 2021 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Virginia Libraries](https://library.virginia.edu).\n",
    "\n",
    "\n",
    "This notebook is adapted by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Multilingual NER 2\n",
    "\n",
    "This is lesson 2 of 3 in the educational series on multilingual NER. This notebook is focused on rules-based NER. \n",
    "\n",
    "**Description:** This notebook describes how to:\n",
    "\n",
    "* Use spaCy to do rule-based NER\n",
    "* Create an EntityRuler\n",
    "* Identify Languages of a Corpus\n",
    "\n",
    "**Use case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "\n",
    "* Python Basics ([start learning Python basics](./python-basics-1.ipynb))\n",
    "* [Python intermediate 4](./python-intermediate-4.ipynb)\n",
    "\n",
    "**Knowledge Recommended:** None\n",
    "\n",
    "**Data Format**: .csv\n",
    "\n",
    "**Libraries Used**: spaCy\n",
    "\n",
    "**Research Pipeline**: None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Install required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fd0c5a-201c-4247-9182-7c238fcff3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/homebrew/lib/python3.10/site-packages (3.4.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.10/site-packages (from spacy) (65.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/homebrew/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting es-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/homebrew/lib/python3.10/site-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (65.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.11)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy # for NLP and NER\n",
    "!pip3 install pandas # for working with tabular data in the exercises of this notebook\n",
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "!python3 -m spacy download es_core_news_sm # for Spanish NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa33f3d-8acb-4e7d-9484-adf1b405fff6",
   "metadata": {},
   "source": [
    "# Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1f94f-a290-46a4-b3cb-6efb4e7f1f1c",
   "metadata": {},
   "source": [
    "The spaCy (spelled correctly) library is a robust library for Natural Language Processing. It supports a wide variety of languages with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. \n",
    "\n",
    "Let's see an example of NLP task that spaCy can do for us.\n",
    "\n",
    "## Tokenization\n",
    "Recall that last time we have seen a graph showing the NLP pipelines. A pipeline's purpose is to take input data, perform some sort of operations on that input data, and then output some useful information from the data. On a pipeline, we find the pipes. A pipe is an individual component of a pipeline. Different pipes perform different tasks. After we read in the data from a text file, an essential task of NLP is tokenization. \n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_NLP_pipeline.png' width=700></center>\n",
    "\n",
    "One form of tokenization is **word tokenization**. When we do word tokenization, we break a text up into individual words and punctuations. Another form of tokenization is **sentence tokenization**. Sentence tokenization is precisely the same as word tokenization, except instead of breaking a text up into individual words and punctuations, we break a text up into individual sentences.\n",
    "\n",
    "If you are an English speaker, you may think you do not need spaCy for sentence tokenization, because in English, the end of a sentence is indicated by a period `.`. Why not just use the the built-in `split()` function which allows us to split a text string by the period `.`? \n",
    "\n",
    "This is a ligit question, but simply splitting a text string by the period `.` will run into problems sometimes and spaCy is actually way more smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1972efd0-6796-4f57-84da-d1556bb34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to be split\n",
    "text = \"Martin J. Thompson is known for his writing skills. He is also good at programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eef0746-500d-4b19-859c-a169f95bf728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin J', ' Thompson is known for his writing skills', ' He is also good at programming', '']\n"
     ]
    }
   ],
   "source": [
    "# Split the string by period\n",
    "sents = text.split(\".\")\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03c2e6-ad40-402b-bd56-1a41ec717e3a",
   "metadata": {},
   "source": [
    "We had the unfortunate result of splitting at Martin J. The reason for this is obvious. In English, it is common convention to indicate abbreviation with the same punctuation mark used to indicate the end of a sentence. \n",
    "\n",
    "We can use SpaCy, however, to do sentence tokenization. SpaCy is smart enough to not break at Martin J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1b5d9-916b-4d23-849f-05521020971c",
   "metadata": {},
   "source": [
    "First, let's import the spaCy library. Then, we need to load an NLP model object. To do this, we use the `spacy.load()` function. Here, we load the small English NLP model trained on written web text that includes vocabulary, syntax and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74893c3e-8f54-4ce8-a4a3-a9322cb43219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English NLP model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361eac93-bbe0-4fab-9ef6-09d8b2199a71",
   "metadata": {},
   "source": [
    "We can use this English NLP model to parse a text and create a Doc object. If you need a quick refresh about what classes and object are, you can refer to [Python intermediate 4](./python-intermediate-4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3822520c-2982-41e4-9bf8-73502574ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the English model to parse the text we created\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bee644",
   "metadata": {},
   "source": [
    "There is a lot of data stored in the Doc object. For example, we can iterate over the sentences in the Doc object and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f3b1dd-0d64-488e-b777-a71a5e9f0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson is known for his writing skills.\n",
      "He is also good at programming.\n"
     ]
    }
   ],
   "source": [
    "# Get the sentence tokens in doc\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee61835-ba86-4875-b80d-a70c3941da85",
   "metadata": {},
   "source": [
    "# spaCy's built-in NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbdbc0-64f6-402e-9c8a-05d80476c204",
   "metadata": {},
   "source": [
    "We have seen one example NLP task that spaCy can do for us. Now let's move on to named entity recognition, the NLP task we focus on in this series.\n",
    "\n",
    "SpaCy already has a built NER off the shelf for us to use. \n",
    "\n",
    "We will iterate over the doc object as we did above, but instead of iterating over `doc.sents`, we will iterate over `doc.ents`. For our purposes right now, we simply want to get each entity's text (the string itself) and its corresponding label (note the underscore `_` after label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef33dd7e-e00f-47fb-9df4-2f83b2694111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print out the entities in the doc object together with their labels\n",
    "for ent in doc.ents: # iterate over the entities \n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2f1b1-7fd6-4467-9d55-f026e6b2e47e",
   "metadata": {},
   "source": [
    "As we can see the small English model has correctly identified that Martin J. Thompson is an entity and given it the correct label PERSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5ce23",
   "metadata": {},
   "source": [
    "Of course we have many different kinds of entities. Here is a list of entity labels used by the small English NLP model we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4b1939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of labels in the small English model for NER\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c382a",
   "metadata": {},
   "source": [
    "If you would like to know the meaning of a label, you can use the `explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02bdd129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get what a label means\n",
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942834f",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d68491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file ready.\n"
     ]
    }
   ],
   "source": [
    "### Download the .csv file for this exercise\n",
    "import urllib.request\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_Harry_Potter_1.csv'\n",
    "urllib.request.urlretrieve(url, './data/' + url.rsplit('/', 1)[-1])   \n",
    "print('Sample file ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b41aaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I should've known that you would be here, Prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Good evening, Professor Dumbledore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>The good and the bad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Character                                           Sentence\n",
       "0  Dumbledore  I should've known that you would be here, Prof...\n",
       "1  McGonagall                Good evening, Professor Dumbledore.\n",
       "2  McGonagall                        Are the rumors true, Albus?\n",
       "3  Dumbledore                          I'm afraid so, professor.\n",
       "4  Dumbledore                              The good and the bad."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Take a look at the first five rows of the table\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/NER_Harry_Potter_1.csv', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbce6b3",
   "metadata": {},
   "source": [
    "In this table we find the name of the characters speaking and their speech. \n",
    "\n",
    "Can you make two new columns, \"Entities\" and \"Labels\", such that each row of the \"Entities\" column stores a list of entities found in the sentence in the same row and each row of the \"Labels\" column stores a list of labels for the entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7b4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35545318-8fb3-48ef-bfa1-418f280786b6",
   "metadata": {},
   "source": [
    "# spaCy's EntityRuler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb707b61-895b-47da-b2ec-a94bbf22d28e",
   "metadata": {},
   "source": [
    "Life would be so easy if we could just grab the ready-to-use built-in NER of spaCy and apply it to the large volume of data we have at hand. However, things are not that easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b4c182-a823-4390-bc24-9ff0fd308f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denmark GPE\n",
      "the 14th century DATE\n"
     ]
    }
   ],
   "source": [
    "# Another sample text string\n",
    "text = \"Aars is a small town in Denmark. The town was founded in the 14th century.\"\n",
    "\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8bfb8",
   "metadata": {},
   "source": [
    "We see that the built-in NER failed to identify Aars as an entity of the GPE type. If we do want to extract 'Aars' from the text and give it a label of GPE, what can we do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365df555",
   "metadata": {},
   "source": [
    "## Add EntityRuler as a new pipe\n",
    "\n",
    "Recall that we have talked about the pipes in a pipeline at the beginning of this lesson. In the case of spaCy, there are a few different pipes that perform different tasks. The tokenizer tokenizes the text into individual tokens; the parser parses the text, and the NER identifies entities and labels them accordingly. When we create a Doc object, all of this data is stored in the Doc object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22121bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the current pipes\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c60267",
   "metadata": {},
   "source": [
    "The EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. In order to extract the target entities and label them successfully, we can create an EntityRuler, give it some instructions, and then add it to the spaCy pipeline as a new pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14c5d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Aars\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5c4fd",
   "metadata": {},
   "source": [
    "After we add the EntityRuler, we can use the new pipeline to do NER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca606d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aars GPE\n",
      "Denmark GPE\n",
      "the 14th century DATE\n"
     ]
    }
   ],
   "source": [
    "# Use the new model to parse the text and create a new Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities and print them out\n",
    "for ent in doc.ents: \n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f10ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the pipes in the new pipeline\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfa7ab",
   "metadata": {},
   "source": [
    "## The importance of order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097340de",
   "metadata": {},
   "source": [
    "It is important to remember that pipelines are sequential. This means that components earlier in a pipeline affect what later components receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "958284e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xiong'an ORG\n",
      "Beijing GPE\n"
     ]
    }
   ],
   "source": [
    "# Use the new model to parse a new text string\n",
    "text = \"Xiong'an is a satellite city of Beijing.\"\n",
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp1(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b17441",
   "metadata": {},
   "source": [
    "Xiong'an is a name of a city. We would want to label it as GPE, not ORG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b6359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xiong'an ORG\n",
      "Beijing GPE\n"
     ]
    }
   ],
   "source": [
    "# Create the EntityRuler\n",
    "ruler = nlp1.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Xiong'an\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Get the entities\n",
    "doc = nlp1(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b274b0b",
   "metadata": {},
   "source": [
    "Why do we still mislabel Xiong'an? This is because when we add the EntityRuler as a new pipe, it gets added at the end of the pipeline automatically. That means the EntityRuler will come after the built-in NER in spaCy. Since NER is a hard classification task, an entity that gets labeled will not be relabeled. If Xiong'an is labeled already by the built-in NER as ORG, it will not be relabeled by the EntityRuler that comes after. In order to give the EntityRuler primacy, we will have to put it in a position before the built-in NER when we add it so that it takes primacy over the built-in NER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4d24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xiong'an GPE\n",
      "Beijing GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create the EntityRuler and add it to the model\n",
    "ruler = nlp2.add_pipe(\"entity_ruler\", before='ner') # specify that the EntityRuler comes before built-in NER\n",
    "\n",
    "# Add the new patterns to the ruler\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Xiong'an\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Use the new model to parse the text\n",
    "doc = nlp2(text)\n",
    "\n",
    "# Get the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36cd8c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'entity_ruler': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EntityRuler comes before the built in ner in nlp2\n",
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291db136",
   "metadata": {},
   "source": [
    "So far, we only add exact strings to our EntityRuler. However, when we talk about patterns, we usually talk about more abstract patterns, not fixed strings. In the following, we will see an example where we write a regular expression pattern and add it to the EntityRuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6041af",
   "metadata": {},
   "source": [
    "## Write a regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e90539",
   "metadata": {},
   "source": [
    "Suppose we have a text written in English, except that the names are written in Latin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6d17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English text with Latin names\n",
    "text = \"Marius was a consul in Rome. Marie is the vocative form.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1a67b",
   "metadata": {},
   "source": [
    "We could write a function that captures the different forms of the name Marius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b63d65a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'PERSON', 'pattern': 'Marius'},\n",
       " {'label': 'PERSON', 'pattern': 'Marii'},\n",
       " {'label': 'PERSON', 'pattern': 'Mario'},\n",
       " {'label': 'PERSON', 'pattern': 'Marium'},\n",
       " {'label': 'PERSON', 'pattern': 'Marie'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that captures the pattern for the Latin name Marius\n",
    "def pattern(root):\n",
    "    endings = [\"us\", \"i\", \"o\", \"um\", \"e\"] # the different endings of the name \n",
    "    patterns = [{\"label\": \"PERSON\", \"pattern\": root+ending} for ending in endings]\n",
    "    return patterns\n",
    "marius = pattern(\"Mari\")\n",
    "marius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed90e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty English NLP model\n",
    "nlp_latin = spacy.blank(\"en\")\n",
    "\n",
    "# Add an EntityRuler\n",
    "nlp_latin_ruler = nlp_latin.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# add the pattern for the Latin name Marius to the EntityRuler\n",
    "nlp_latin_ruler.add_patterns(marius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5a5bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marius PERSON\n",
      "Marie PERSON\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc_latin = nlp_latin(text)\n",
    "\n",
    "# Iterate over the entities in Doc object and print them out\n",
    "for ent in doc_latin.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541aa75f",
   "metadata": {},
   "source": [
    "We could also use regex to help us write the pattern. If you would like to have a quick refresh of regular expressions, you can refer to the notebook [Regular Expressions](./regular-expressions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cd722c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marius PERSON\n",
      "Marie PERSON\n"
     ]
    }
   ],
   "source": [
    "# Write a function which returns the pattern for Latin name Marius\n",
    "def latin_roots(root):\n",
    "    return [{\"label\": \"PERSON\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\" + root + r\"(us|i|o|um|e)$\"}}]}]\n",
    "\n",
    "# Save the pattern to the variable marius2\n",
    "marius2 = latin_roots(\"Mari\")\n",
    "\n",
    "# Create a blank English NLP model\n",
    "nlp_latin2 = spacy.blank(\"en\")\n",
    "\n",
    "# Add an EntityRuler to the model\n",
    "nlp_latin_ruler2 = nlp_latin2.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the pattern for Latin name Marius to the EntityRuler\n",
    "nlp_latin_ruler2.add_patterns(marius2)\n",
    "\n",
    "# Text to be parsed\n",
    "text = \"Marius was a consul in Rome. Marie is the vocative form. Caesar was a dictator.\"\n",
    "\n",
    "# Create a Doc object using the new model with the regex pattern in EntityRuler\n",
    "doc_latin2 = nlp_latin2(text)\n",
    "\n",
    "# Iterate over the entities and print them out\n",
    "for ent in doc_latin2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a8819",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "You have seen in coding challenge one that the off-the-shelf NER of Spacy mislabeled some entities. For example, \"Hagrid\", a person's name, is labeled as ORG. Suppose you have a file with all the characters' names in it. Can you make an EntityRuler and add it to the SpaCy pipeline so that all the person names will be labeled 'PERSON'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27ccd4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file ready.\n"
     ]
    }
   ],
   "source": [
    "### Download the .csv file for this exercise\n",
    "import urllib.request\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_HarryPotter_Characters.csv'\n",
    "urllib.request.urlretrieve(url, './data/' + url.rsplit('/', 1)[-1])   \n",
    "print('Sample file ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2cdee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Job</th>\n",
       "      <th>House</th>\n",
       "      <th>Wand</th>\n",
       "      <th>Patronus</th>\n",
       "      <th>Species</th>\n",
       "      <th>Blood status</th>\n",
       "      <th>Hair colour</th>\n",
       "      <th>Eye colour</th>\n",
       "      <th>Loyalty</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Birth</th>\n",
       "      <th>Death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry James Potter</td>\n",
       "      <td>Male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>11\"  Holly  phoenix feather</td>\n",
       "      <td>Stag</td>\n",
       "      <td>Human</td>\n",
       "      <td>Half-blood</td>\n",
       "      <td>Black</td>\n",
       "      <td>Bright green</td>\n",
       "      <td>Albus Dumbledore | Dumbledore's Army | Order o...</td>\n",
       "      <td>Parseltongue| Defence Against the Dark Arts | ...</td>\n",
       "      <td>31 July 1980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ronald Bilius Weasley</td>\n",
       "      <td>Male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>12\" Ash unicorn tail hair</td>\n",
       "      <td>Jack Russell terrier</td>\n",
       "      <td>Human</td>\n",
       "      <td>Pure-blood</td>\n",
       "      <td>Red</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Dumbledore's Army | Order of the Phoenix | Hog...</td>\n",
       "      <td>Wizard chess | Quidditch goalkeeping</td>\n",
       "      <td>1 March 1980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hermione Jean Granger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>10¾\"  vine wood dragon heartstring</td>\n",
       "      <td>Otter</td>\n",
       "      <td>Human</td>\n",
       "      <td>Muggle-born</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Dumbledore's Army | Order of the Phoenix | Hog...</td>\n",
       "      <td>Almost everything</td>\n",
       "      <td>19 September, 1979</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Albus Percival Wulfric Brian Dumbledore</td>\n",
       "      <td>Male</td>\n",
       "      <td>Headmaster</td>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>15\" Elder Thestral tail hair core</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Human</td>\n",
       "      <td>Half-blood</td>\n",
       "      <td>Silver| formerly auburn</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Dumbledore's Army | Order of the Phoenix | Hog...</td>\n",
       "      <td>Considered by many to be one of the most power...</td>\n",
       "      <td>Late August 1881</td>\n",
       "      <td>30 June, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rubeus Hagrid</td>\n",
       "      <td>Male</td>\n",
       "      <td>Keeper of Keys and Grounds | Professor of Care...</td>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>16\"  Oak unknown core</td>\n",
       "      <td>None</td>\n",
       "      <td>Half-Human/Half-Giant</td>\n",
       "      <td>Part-Human (Half-giant)</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>Albus Dumbledore | Order of the Phoenix | Hogw...</td>\n",
       "      <td>Resistant to stunning spells| above average st...</td>\n",
       "      <td>6 December 1928</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                     Name  Gender  \\\n",
       "0   1                       Harry James Potter    Male   \n",
       "1   2                    Ronald Bilius Weasley    Male   \n",
       "2   3                    Hermione Jean Granger  Female   \n",
       "3   4  Albus Percival Wulfric Brian Dumbledore    Male   \n",
       "4   5                            Rubeus Hagrid    Male   \n",
       "\n",
       "                                                 Job       House  \\\n",
       "0                                            Student  Gryffindor   \n",
       "1                                            Student  Gryffindor   \n",
       "2                                            Student  Gryffindor   \n",
       "3                                         Headmaster  Gryffindor   \n",
       "4  Keeper of Keys and Grounds | Professor of Care...  Gryffindor   \n",
       "\n",
       "                                 Wand              Patronus  \\\n",
       "0         11\"  Holly  phoenix feather                  Stag   \n",
       "1          12\" Ash unicorn tail hair   Jack Russell terrier   \n",
       "2  10¾\"  vine wood dragon heartstring                 Otter   \n",
       "3   15\" Elder Thestral tail hair core               Phoenix   \n",
       "4               16\"  Oak unknown core                  None   \n",
       "\n",
       "                 Species             Blood status              Hair colour  \\\n",
       "0                  Human               Half-blood                    Black   \n",
       "1                  Human               Pure-blood                      Red   \n",
       "2                  Human              Muggle-born                    Brown   \n",
       "3                  Human               Half-blood  Silver| formerly auburn   \n",
       "4  Half-Human/Half-Giant  Part-Human (Half-giant)                    Black   \n",
       "\n",
       "     Eye colour                                            Loyalty  \\\n",
       "0  Bright green  Albus Dumbledore | Dumbledore's Army | Order o...   \n",
       "1          Blue  Dumbledore's Army | Order of the Phoenix | Hog...   \n",
       "2         Brown  Dumbledore's Army | Order of the Phoenix | Hog...   \n",
       "3          Blue  Dumbledore's Army | Order of the Phoenix | Hog...   \n",
       "4         Black  Albus Dumbledore | Order of the Phoenix | Hogw...   \n",
       "\n",
       "                                              Skills               Birth  \\\n",
       "0  Parseltongue| Defence Against the Dark Arts | ...        31 July 1980   \n",
       "1               Wizard chess | Quidditch goalkeeping        1 March 1980   \n",
       "2                                  Almost everything  19 September, 1979   \n",
       "3  Considered by many to be one of the most power...    Late August 1881   \n",
       "4  Resistant to stunning spells| above average st...     6 December 1928   \n",
       "\n",
       "            Death  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3  30 June, 1997   \n",
       "4             NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data from the character csv file\n",
    "chars_df = pd.read_csv('./data/NER_HarryPotter_Characters.csv', delimiter=';')\n",
    "\n",
    "# Take a look at the first five rows\n",
    "chars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1c2f5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>split_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry James Potter</td>\n",
       "      <td>[Harry, James, Potter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ronald Bilius Weasley</td>\n",
       "      <td>[Ronald, Bilius, Weasley]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hermione Jean Granger</td>\n",
       "      <td>[Hermione, Jean, Granger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albus Percival Wulfric Brian Dumbledore</td>\n",
       "      <td>[Albus, Percival, Wulfric, Brian, Dumbledore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rubeus Hagrid</td>\n",
       "      <td>[Rubeus, Hagrid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Wilhelmina Grubbly-Plank</td>\n",
       "      <td>[Wilhelmina, Grubbly-Plank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Fenrir Greyback</td>\n",
       "      <td>[Fenrir, Greyback]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Gellert Grindelwald</td>\n",
       "      <td>[Gellert, Grindelwald]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Dobby</td>\n",
       "      <td>[Dobby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Kreacher</td>\n",
       "      <td>[Kreacher]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name  \\\n",
       "0                         Harry James Potter   \n",
       "1                      Ronald Bilius Weasley   \n",
       "2                      Hermione Jean Granger   \n",
       "3    Albus Percival Wulfric Brian Dumbledore   \n",
       "4                              Rubeus Hagrid   \n",
       "..                                       ...   \n",
       "135                 Wilhelmina Grubbly-Plank   \n",
       "136                          Fenrir Greyback   \n",
       "137                      Gellert Grindelwald   \n",
       "138                                    Dobby   \n",
       "139                                 Kreacher   \n",
       "\n",
       "                                        split_name  \n",
       "0                           [Harry, James, Potter]  \n",
       "1                        [Ronald, Bilius, Weasley]  \n",
       "2                        [Hermione, Jean, Granger]  \n",
       "3    [Albus, Percival, Wulfric, Brian, Dumbledore]  \n",
       "4                                 [Rubeus, Hagrid]  \n",
       "..                                             ...  \n",
       "135                    [Wilhelmina, Grubbly-Plank]  \n",
       "136                             [Fenrir, Greyback]  \n",
       "137                         [Gellert, Grindelwald]  \n",
       "138                                        [Dobby]  \n",
       "139                                     [Kreacher]  \n",
       "\n",
       "[140 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all parts from a character's name\n",
    "chars_df = chars_df[['Name']] \n",
    "chars_df['split_name'] = chars_df['Name'].str.split(' ')\n",
    "chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f7c43f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Graham',\n",
       " 'George',\n",
       " 'Edgar',\n",
       " 'Myrtle',\n",
       " 'Tom',\n",
       " 'Marietta',\n",
       " 'Viktor',\n",
       " 'Sybill',\n",
       " 'Hermione',\n",
       " 'Michael',\n",
       " 'Cuthbert',\n",
       " 'Scamander',\n",
       " 'Jordan\\xa0',\n",
       " 'Anthony',\n",
       " 'Abbott',\n",
       " 'Weasley',\n",
       " 'Dudley',\n",
       " 'Dobby',\n",
       " 'Quirinus',\n",
       " 'Yaxley',\n",
       " 'Helga',\n",
       " 'Quirrell',\n",
       " 'Lovegood',\n",
       " 'Bones',\n",
       " 'Bulstrode',\n",
       " 'Friar',\n",
       " 'Boot',\n",
       " 'Binns',\n",
       " 'Mundungus',\n",
       " 'McGonagall',\n",
       " 'Hufflepuff',\n",
       " 'Granger',\n",
       " 'Fat',\n",
       " 'Fenrir',\n",
       " 'Finch-Fletchley',\n",
       " 'Helena',\n",
       " 'Lestrange',\n",
       " 'Bellatrix',\n",
       " 'Hannah',\n",
       " 'Pomfrey',\n",
       " 'Cornelius',\n",
       " 'Newton',\n",
       " 'Charles',\n",
       " 'Lucius',\n",
       " 'Antonin',\n",
       " 'Fletcher',\n",
       " 'Marcus',\n",
       " 'Dolohov',\n",
       " 'Trelawney',\n",
       " 'Wood',\n",
       " 'Walden',\n",
       " 'Cormac',\n",
       " 'Grubbly-Plank',\n",
       " 'Filch',\n",
       " 'Katie',\n",
       " 'Rufus',\n",
       " 'Bell',\n",
       " 'Zacharias',\n",
       " 'Ginevra',\n",
       " 'Lupin',\n",
       " 'Macnair',\n",
       " 'Firenze',\n",
       " 'Molly',\n",
       " 'Greyback',\n",
       " 'Pansy',\n",
       " 'Umbridge',\n",
       " 'Slughorn',\n",
       " 'Macmillan',\n",
       " 'Dennis',\n",
       " 'Benjy',\n",
       " 'Rolanda',\n",
       " 'Gilderoy',\n",
       " 'Percy',\n",
       " 'Hagrid',\n",
       " 'Burbage',\n",
       " 'Scrimgeour',\n",
       " 'Moody',\n",
       " 'Romilda',\n",
       " 'Kreacher',\n",
       " 'Dumbledore',\n",
       " 'Clearwater',\n",
       " 'Belby',\n",
       " 'Goyle',\n",
       " 'Delacour',\n",
       " 'Meadowes',\n",
       " 'Harry',\n",
       " 'Parvati',\n",
       " 'Igor',\n",
       " 'Gideon',\n",
       " 'Corner',\n",
       " 'Lee',\n",
       " 'Gellert',\n",
       " 'Alecto',\n",
       " 'Emmeline',\n",
       " 'Maxime',\n",
       " 'Amos',\n",
       " 'Minerva',\n",
       " 'Terry',\n",
       " 'Vance',\n",
       " 'Filius',\n",
       " 'Podmore',\n",
       " 'Dorcas',\n",
       " 'Cho',\n",
       " 'Colin',\n",
       " 'Gabrielle',\n",
       " 'Grindelwald',\n",
       " 'Montague',\n",
       " 'Rowena',\n",
       " 'Scorpius',\n",
       " 'Carrow',\n",
       " 'Malfoy',\n",
       " 'Aurora',\n",
       " 'Davies',\n",
       " 'Alicia',\n",
       " 'Ernest',\n",
       " 'Argus',\n",
       " 'Karkaroff',\n",
       " 'Frank',\n",
       " 'Ravenclaw',\n",
       " 'Finnigan',\n",
       " 'Creevey',\n",
       " 'Dedalus',\n",
       " 'Petunia',\n",
       " 'Bill',\n",
       " 'Arthur',\n",
       " 'Nymphadora',\n",
       " 'Flitwick',\n",
       " 'Elphias',\n",
       " 'Dolores',\n",
       " 'Sr.',\n",
       " 'Blaise',\n",
       " 'Doge',\n",
       " 'Mimsy-Porpington',\n",
       " 'Theodore',\n",
       " 'Johnson',\n",
       " 'Augustus',\n",
       " 'Poppy',\n",
       " 'Vincent',\n",
       " 'Lockhart',\n",
       " 'Amycus',\n",
       " 'Goldstein',\n",
       " 'Diggory',\n",
       " 'Corban',\n",
       " 'Aberforth',\n",
       " 'Diggle',\n",
       " 'Lily',\n",
       " 'Vane',\n",
       " 'Justin',\n",
       " 'Salazar',\n",
       " 'Sprout',\n",
       " 'Chang',\n",
       " 'Nicholas',\n",
       " 'Vernon',\n",
       " 'Regulus',\n",
       " 'Wilhelmina',\n",
       " 'Patil',\n",
       " 'Jr.',\n",
       " 'Flint',\n",
       " 'Cedric',\n",
       " 'Susan',\n",
       " 'Kingsley',\n",
       " 'Slytherin',\n",
       " 'McLaggen',\n",
       " 'Fudge',\n",
       " 'Gryffindor',\n",
       " 'Lavender',\n",
       " 'Vector',\n",
       " 'Prewett',\n",
       " 'Peter',\n",
       " 'Smith',\n",
       " 'Fleur',\n",
       " 'Pomona',\n",
       " 'Millicent',\n",
       " 'Zabini',\n",
       " 'Madame',\n",
       " 'Marge',\n",
       " 'Ollivander',\n",
       " 'Rubeus',\n",
       " 'Horace',\n",
       " 'Ronald',\n",
       " 'Jones',\n",
       " 'Hooch',\n",
       " 'Seamus',\n",
       " 'Alice',\n",
       " 'Marlene',\n",
       " 'Granger-Weasley',\n",
       " 'Albus',\n",
       " 'Brown',\n",
       " 'Longbottom',\n",
       " 'Riddle',\n",
       " 'Barty',\n",
       " 'Gregory',\n",
       " 'Shacklebolt',\n",
       " 'Edward',\n",
       " 'James',\n",
       " 'Thomas',\n",
       " 'Oliver',\n",
       " 'Tonks',\n",
       " 'Rodolphus',\n",
       " 'Draco',\n",
       " 'Hestia',\n",
       " 'Parkinson',\n",
       " 'Roger',\n",
       " 'Sturgis',\n",
       " 'Fenwick',\n",
       " 'Dean',\n",
       " 'Padma',\n",
       " 'Narcissa',\n",
       " 'Severus',\n",
       " 'Alastor',\n",
       " 'Sirius',\n",
       " 'Fred',\n",
       " 'Irma',\n",
       " 'Garrick',\n",
       " 'Septima',\n",
       " 'Baron',\n",
       " 'Rookwood',\n",
       " 'Black',\n",
       " 'Dursley',\n",
       " 'Rose',\n",
       " 'Pince',\n",
       " 'Charity',\n",
       " 'Potter',\n",
       " 'Spinnet',\n",
       " 'Neville',\n",
       " 'Bloody',\n",
       " 'Nott',\n",
       " 'Crabbe',\n",
       " 'Remus',\n",
       " 'Pettigrew',\n",
       " 'Fabian',\n",
       " 'McKinnon',\n",
       " 'Krum',\n",
       " 'Luna',\n",
       " 'Godric',\n",
       " 'Edgecombe',\n",
       " 'Snape',\n",
       " 'Sinistra',\n",
       " 'Penelope',\n",
       " 'Angelina']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first names and last names of the characters\n",
    "chars_df['first_name'] = chars_df['split_name'].str[0]\n",
    "chars_df['last_name'] = chars_df['split_name'].str[-1]\n",
    "\n",
    "first_names = chars_df['first_name'].unique().tolist() # Put all unique first names in a list\n",
    "last_names = chars_df['last_name'].unique().tolist() # Put all unique last names in a list\n",
    "\n",
    "names = list(set(first_names) | set(last_names)) # the vertical bar | gives us the union of the two sets\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464976ed",
   "metadata": {},
   "source": [
    "Create an EntityRuler. In the ruler, add all characters' names as pattern and specify the label for them as \"PERSON\". Add the ruler as a new pipe. Last, add two new columns to the dataframe you created from the original NER_Harry_Potter_1.csv file, one storing the entities found in each sentence and one storing the labels for the entities. This time, all characters' names should be correctly labeled as \"PERSON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e32c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd278cd-abb2-4069-9017-518a2aa90922",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detecting languages in texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335be5f-5a9a-4357-aa82-adcec964ed33",
   "metadata": {},
   "source": [
    "When we work with a multilingual corpus, we will first want to know the different languages used in the corpus. There are different approaches to do this. In this section, I will introduce a third-party library Lingua for language detection. Currently, 75 languages are supported by Lingua."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6a840-d056-486b-9ca9-7e907fc6c48f",
   "metadata": {},
   "source": [
    "## Language detection with Lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33e29cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lingua-language-detector in /opt/homebrew/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /opt/homebrew/lib/python3.10/site-packages (from lingua-language-detector) (1.24.1)\n",
      "Requirement already satisfied: regex<2023.0.0,>=2022.10.31 in /opt/homebrew/lib/python3.10/site-packages (from lingua-language-detector) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "# Install language detector\n",
    "\n",
    "!pip3 install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70d3157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the language detector builder\n",
    "from lingua import LanguageDetectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "543be10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41924cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language.ENGLISH"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"This is an English text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06d2c77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language.PORTUGUESE"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"Este é um outro texto sem idioma especificado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d39e860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language.CHINESE"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"这是一句中文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b4393",
   "metadata": {},
   "source": [
    "Sometimes you may already know the range of languages in your corpus. You just want to identify the language for each document. In this case, you could narrow down the language detector to only a few languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a994e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConfidenceValue(language=Language.ENGLISH, value=0.8163394729525284),\n",
       " ConfidenceValue(language=Language.GERMAN, value=0.10768916002370368),\n",
       " ConfidenceValue(language=Language.SPANISH, value=0.04163053407860697),\n",
       " ConfidenceValue(language=Language.FRENCH, value=0.03434083294516089)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a language detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "# Use the detector to decide between the given languages \n",
    "detector.compute_language_confidence_values(\"This is an English text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519ab65-c5bf-4fc4-8560-4623d6a54b03",
   "metadata": {},
   "source": [
    "## Multiple languages in the same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a90f4-3566-4efd-86bb-3e8910017f15",
   "metadata": {},
   "source": [
    "The examples we go over just now assume that only one language is used in each document. However, the language detector we build cannot reliably detect multiple languages, because it will only output one language for a text by default. What if our text has multiple languages, such as the example below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2eed8aa5-d63a-48ec-9adb-28ab3c1b6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a text string with multiple languages \n",
    "large_text = '''This is a text where the first line is in English.\n",
    "Maar de tweede regel is in het Nederlands. \n",
    "Dies ist ein deutscher Text.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43054ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "languages = [Language.ENGLISH, Language.DUTCH, Language.GERMAN]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53c55f-eba2-4590-84cf-4f3ec7328cf5",
   "metadata": {},
   "source": [
    "If we run the detector over this text, we get the following output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ff11442-cabf-4304-a3db-7520deb7aca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language.DUTCH"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the detector to decide the language of the text\n",
    "detector.detect_language_of(large_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239812ea",
   "metadata": {},
   "source": [
    "By default, Lingua returns the most likely language for a given input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51c2f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH: 0.64\n",
      "GERMAN: 0.26\n",
      "ENGLISH: 0.11\n"
     ]
    }
   ],
   "source": [
    "# Get the likelihood of the decision\n",
    "confidence_values = detector.compute_language_confidence_values(large_text)\n",
    "for language, value in confidence_values:\n",
    "    print(f\"{language.name}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113331fe-bdc1-4c29-a13b-cbc45c42729c",
   "metadata": {},
   "source": [
    "But this text has multiple languages. In this example text, each sentence is written in a different language. Therefore, we need to get each sentence string and run the detector over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bb39473-3835-4e1d-b77a-446fe6a131bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is a text where the first line is in English.\n",
      "Language.ENGLISH\n",
      "Sentence: Maar de tweede regel is in het Nederlands.\n",
      "Language.DUTCH\n",
      "Sentence: Dies ist ein deutscher Text.\n",
      "Language.GERMAN\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object \n",
    "doc = nlp(large_text)\n",
    "\n",
    "# Iterate over each sentence and run the detector over it\n",
    "for sent in doc.sents:\n",
    "    print(f\"Sentence: {sent.text.strip()}\")\n",
    "    print(detector.detect_language_of(sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d126fba",
   "metadata": {},
   "source": [
    "# Bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bc8cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A document that has two languages, English and Spanish\n",
    "multilingual_document = \"\"\"This is a story about Margaret who speaks Spanish. \n",
    "'Juan Miguel es mi amigo y tiene veinte años.' Margeret said to her friend Sarah.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25275e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19b96aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant models\n",
    "english_nlp = spacy.load(\"en_core_web_sm\") # for English\n",
    "spanish_nlp = spacy.load(\"es_core_news_sm\") # for Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26ecf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an NLP model and create a Doc object\n",
    "multi_nlp = spacy.blank('en')\n",
    "\n",
    "# Add sentencizer\n",
    "multi_nlp.add_pipe('sentencizer')\n",
    "\n",
    "# Create a Doc object\n",
    "multi_doc = multi_nlp(multilingual_document.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a55bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margaret PERSON\n",
      "Spanish LANGUAGE\n",
      "Juan Miguel PER\n",
      "Margeret ORG\n",
      "Sarah PERSON\n"
     ]
    }
   ],
   "source": [
    "# Switching between languages with conditionals\n",
    "\n",
    "for sent in multi_doc.sents:\n",
    "    if detector.detect_language_of(sent.text).name == \"ENGLISH\":\n",
    "        nested_doc = english_nlp(sent.text.strip())\n",
    "    elif detector.detect_language_of(sent.text).name == \"SPANISH\":\n",
    "        nested_doc = spanish_nlp(sent.text.strip())\n",
    "    for ent in nested_doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720817e",
   "metadata": {},
   "source": [
    "___\n",
    "# Lesson Complete\n",
    "Congratulations! You have completed *NER 2*.\n",
    "\n",
    "## Start Next Lesson: [NER 3](./NER-3.ipynb)\n",
    "\n",
    "## Coding Challenge! Solutions\n",
    "\n",
    "There are often many ways to solve programming problems. Here are a few possible ways to solve the challenges, but there are certainly more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e7680",
   "metadata": {},
   "source": [
    "**Exercise 1** In the code cell below, we add two new columns to the dataframe created from NER_Harry_Potter_1.csv, one storing the entities found in each sentence using the off-the-shelf NER in spaCy, one storing the labels for the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b026ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I should've known that you would be here, Prof...</td>\n",
       "      <td>[McGonagall]</td>\n",
       "      <td>[PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Good evening, Professor Dumbledore.</td>\n",
       "      <td>[evening, Dumbledore]</td>\n",
       "      <td>[TIME, PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "      <td>[Albus]</td>\n",
       "      <td>[GPE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Hagrid</td>\n",
       "      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n",
       "      <td>[Harry, Dudley]</td>\n",
       "      <td>[PERSON, PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>Harry</td>\n",
       "      <td>But Hagrid, we're not allowed to do magic away...</td>\n",
       "      <td>[Hagrid]</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Hagrid</td>\n",
       "      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>Hermione</td>\n",
       "      <td>Feels strange to be going home, doesn't it?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>Harry</td>\n",
       "      <td>I'm not going home. Not really.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1587 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Character                                           Sentence  \\\n",
       "0     Dumbledore  I should've known that you would be here, Prof...   \n",
       "1     McGonagall                Good evening, Professor Dumbledore.   \n",
       "2     McGonagall                        Are the rumors true, Albus?   \n",
       "3     Dumbledore                          I'm afraid so, professor.   \n",
       "4     Dumbledore                              The good and the bad.   \n",
       "...          ...                                                ...   \n",
       "1582      Hagrid  Oh, listen, Harry, if that dolt of a cousin of...   \n",
       "1583       Harry  But Hagrid, we're not allowed to do magic away...   \n",
       "1584      Hagrid  I do. But your cousin don't, do he? Eh? Off yo...   \n",
       "1585    Hermione        Feels strange to be going home, doesn't it?   \n",
       "1586       Harry                    I'm not going home. Not really.   \n",
       "\n",
       "                   Entities            Labels  \n",
       "0              [McGonagall]          [PERSON]  \n",
       "1     [evening, Dumbledore]    [TIME, PERSON]  \n",
       "2                   [Albus]             [GPE]  \n",
       "3                        []                []  \n",
       "4                        []                []  \n",
       "...                     ...               ...  \n",
       "1582        [Harry, Dudley]  [PERSON, PERSON]  \n",
       "1583               [Hagrid]             [ORG]  \n",
       "1584                     []                []  \n",
       "1585                     []                []  \n",
       "1586                     []                []  \n",
       "\n",
       "[1587 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('./data/NER_Harry_Potter_1.csv', delimiter=';')\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Make two new columns to store entities and labels\n",
    "df['Entities'] = df['Sentence'].apply(lambda r: [ent.text for ent in nlp(r).ents])\n",
    "df['Labels'] = df['Sentence'].apply(lambda r: [ent.label_ for ent in nlp(r).ents])\n",
    "\n",
    "# Take a look at the updated df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e6046",
   "metadata": {},
   "source": [
    "**Exercise 2** In the code cell below, we create an EntityRuler. We get all the character names and specify their label as \"PERSON\" in the ruler. We add this EntityRuler to the pipeline. Last, we add two new columns to the dataframe created from NER_Harry_Potter_1.csv, one storing the entities found in each sentence using the updated pipeline, one storing the labels for the entities. This time, all character names are correctly labeled as \"PERSON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5061a795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I should've known that you would be here, Prof...</td>\n",
       "      <td>[McGonagall]</td>\n",
       "      <td>[PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Good evening, Professor Dumbledore.</td>\n",
       "      <td>[evening, Dumbledore]</td>\n",
       "      <td>[TIME, PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McGonagall</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "      <td>[Albus]</td>\n",
       "      <td>[PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dumbledore</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Hagrid</td>\n",
       "      <td>Oh, listen, Harry, if that dolt of a cousin of...</td>\n",
       "      <td>[Harry, Dudley]</td>\n",
       "      <td>[PERSON, PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>Harry</td>\n",
       "      <td>But Hagrid, we're not allowed to do magic away...</td>\n",
       "      <td>[Hagrid]</td>\n",
       "      <td>[PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Hagrid</td>\n",
       "      <td>I do. But your cousin don't, do he? Eh? Off yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>Hermione</td>\n",
       "      <td>Feels strange to be going home, doesn't it?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>Harry</td>\n",
       "      <td>I'm not going home. Not really.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1587 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Character                                           Sentence  \\\n",
       "0     Dumbledore  I should've known that you would be here, Prof...   \n",
       "1     McGonagall                Good evening, Professor Dumbledore.   \n",
       "2     McGonagall                        Are the rumors true, Albus?   \n",
       "3     Dumbledore                          I'm afraid so, professor.   \n",
       "4     Dumbledore                              The good and the bad.   \n",
       "...          ...                                                ...   \n",
       "1582      Hagrid  Oh, listen, Harry, if that dolt of a cousin of...   \n",
       "1583       Harry  But Hagrid, we're not allowed to do magic away...   \n",
       "1584      Hagrid  I do. But your cousin don't, do he? Eh? Off yo...   \n",
       "1585    Hermione        Feels strange to be going home, doesn't it?   \n",
       "1586       Harry                    I'm not going home. Not really.   \n",
       "\n",
       "                   Entities            Labels  \n",
       "0              [McGonagall]          [PERSON]  \n",
       "1     [evening, Dumbledore]    [TIME, PERSON]  \n",
       "2                   [Albus]          [PERSON]  \n",
       "3                        []                []  \n",
       "4                        []                []  \n",
       "...                     ...               ...  \n",
       "1582        [Harry, Dudley]  [PERSON, PERSON]  \n",
       "1583               [Hagrid]          [PERSON]  \n",
       "1584                     []                []  \n",
       "1585                     []                []  \n",
       "1586                     []                []  \n",
       "\n",
       "[1587 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data from the character csv file\n",
    "chars_df = pd.read_csv('./data/NER_HarryPotter_Characters.csv', delimiter=';')\n",
    "\n",
    "# Remove the irrelavant columns and only maintain the 'Name' column\n",
    "chars_df = chars_df[['Name']] \n",
    "\n",
    "# Create a new column storing the parts of each name\n",
    "chars_df['split_name'] = chars_df['Name'].str.split(' ')\n",
    "\n",
    "# Get the first names and last names of the characters\n",
    "chars_df['first_name'] = chars_df['split_name'].str[0]\n",
    "chars_df['last_name'] = chars_df['split_name'].str[-1]\n",
    "first_names = chars_df['first_name'].unique().tolist() # Put all unique first names in a list\n",
    "last_names = chars_df['last_name'].unique().tolist() # Put all unique last names in a list\n",
    "\n",
    "# Get all unique names and put them in a list\n",
    "names = list(set(first_names) | set(last_names)) # the vertical bar | gives us the union of the two sets\n",
    "\n",
    "# Load the English model\n",
    "nlp_ex = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a new EntityRuler and add it as a new pipe\n",
    "ruler = nlp_ex.add_pipe(\"entity_ruler\", before='ner')\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": f\"{name}\"} for name in names]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Read in the data from the character and speech file\n",
    "df = pd.read_csv('./data/NER_Harry_Potter_1.csv', delimiter=';')\n",
    "\n",
    "# Make two new columns to store entities and labels\n",
    "df['Entities'] = df['Sentence'].apply(lambda r: [ent.text for ent in nlp_ex(r).ents])\n",
    "df['Labels'] = df['Sentence'].apply(lambda r: [ent.label_ for ent in nlp_ex(r).ents])\n",
    "\n",
    "# Take a look at the updated df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9345c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
