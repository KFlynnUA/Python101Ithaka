{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "This notebook was created by [William Mattingly](https://datascience.si.edu/people/dr-william-mattingly) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "This notebook is adapted by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Multilingual NER 3\n",
    "\n",
    "This is lesson 3 in the educational series on named entity recognition. \n",
    "\n",
    "**Audience:** Teachers / Learners / Researchers\n",
    "\n",
    "**Use case:** Explanation\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "\n",
    "* Python basics ([start learning Python basics](./python-basics-1.ipynb))\n",
    "* [Python intermediate 4](./python-intermediate-4.ipynb) (OOP, classes, instances, inheritance)\n",
    "* [Regular expressions](./regular-expressions.ipynb) (re, character classes)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* Basic file operations ([start learning file operations](./python-intermediate-2.ipynb))\n",
    "* Data cleaning with `Pandas` ([start learning Pandas](./pandas-1.ipynb))\n",
    "\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "* Understand word embeddings\n",
    "* Understand Machine Learning generally\n",
    "* Understanding of how to do NER ML in spaCy 3\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26823dcc",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy # for NLP\n",
    "!pip3 install pandas # for making tabular data\n",
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "!python3 -m spacy download en_core_web_md # for showing the word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac6c55",
   "metadata": {},
   "source": [
    "# Introduction to word embeddings\n",
    "\n",
    "How do we represent word meanings in NLP? Well, we use numerical values. **Word embeddings** are vector representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a2de3",
   "metadata": {},
   "source": [
    "## Distributional hypothesis\n",
    "\n",
    "Word embeddings is inspired by the **distributional hypothesis** was proposed by Harris ([1954](https://doi.org/10.1080/00437956.1954.11659520)). This theory could be summarized as: words that have similar context will have similar meanings.\n",
    "\n",
    "What does \"context\" mean in word mebeddings? Basically, \"context\" means the neighboring words of a target word. \n",
    "\n",
    "Consider the following example. If we choose \"village\" as the target word and choose a fixed size context window of 2, the two words before \"village\" and the two words after \"village\" will constitute the context of the target word.\n",
    "\n",
    "Treblinka is **a small** **<span style=\"color: blue;\">village</span>** **in Poland.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39b123",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Google’s pre-trained word2vec model includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features, which means each of the 3 million words in the vocabulary is represented by a vector with 300 floating numbers. Word2Vec is one of the most popular techniques to learn word embeddings.\n",
    "\n",
    "The training samples are the (target, context) pairs from the text data. For example, suppose your source text is the sentence \"The quick brown fox jumps over the lazy dog\". If you choose \"quick\" as your target word and have set a context window of size 2, you will get three training samples for it, i.e. (quick, the), (quick, brown) and (quick fox).   \n",
    "\n",
    "**McCormick, C**. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com\n",
    "\n",
    "The word2vec model is trained to accomplish the following task: given the input word $w_{1}$, for each word in our vocab $w_{2}$, how likely $w_{2}$ is a context word of $w_{1}$.\n",
    "\n",
    "The network is going to learn the statistics from the number of times each (target, context) shows up. So, for example, if you have a text about kings, queens and kingdoms, the network is probably going to get many more training samples of (\"King\", \"Queen\") than (\"King\", \"kangaroo\"). Therefore, if you give your trained model the word \"King\" as input, then it will output a much higher probability for \"Queen\" than it will for \"kangaroo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae805",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "\n",
    "We have used the small English model from spaCy. Actually, there are medium size and large size English model from spaCy as well. Both are trained using the word2vec family of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663703eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the medium size English model from spaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Get the word vector for the word \"King\"\n",
    "nlp(\"King\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the vector\n",
    "nlp(\"King\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa846c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"Queen\"\n",
    "nlp(\"King\").similarity(nlp(\"Queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"kangaroo\"\n",
    "nlp(\"King\").similarity(nlp(\"kangaroo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99377411-f756-4003-994d-6a202c51cf6e",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning \n",
    "\n",
    "Machine learning is a branch of artificial intelligence. Traditionally the human writes the rules in a computer system to perform a specific task. In machine learning, we use statistics to write the rules for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7bf26-6a6c-4cfb-9987-04ae0342da31",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41c009-2809-4dc1-8939-dbf3951665df",
   "metadata": {},
   "source": [
    "**Supervised learning** is the process by which a system learns from a set of inputs that have known labels. To train the model, we will need part of the data for training, part for validation and part for testing. A rule of thumb to divide the input data into these categories is:\n",
    "\n",
    "* 20% of all annotated data for testing\n",
    "* For the remaining annotated data\n",
    "    * 80% for training\n",
    "    * 20% for validation\n",
    "\n",
    "### Training and validation\n",
    "\n",
    "The training data is used to hone a statistical model via predetermined algorithms. It does this by making guesses about what the proper labels are. It then checks its accuracy against the correct labels, i.e., the annotated labels, and makes adjustments accordingly.\n",
    "\n",
    "Once it is finished viewing and guessing across all the training data, the first **epoch**, or **iteration** over the data, is finished. At this stage, the model then tests its accuracy against the validation data. The training data is then randomized and given back to the system for x number of epochs. Again, there is no standard for the number of epochs, but a good rule of thumb is to start at 10 and study the results.\n",
    "\n",
    "### Testing\n",
    "Once the model repeats this process for the set number of epochs, it is finished training. The model’s accuracy can then be tested against the testing dataset to see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68af31-4703-4750-865a-818142566dd5",
   "metadata": {},
   "source": [
    "# Create a custom model in SpaCy\n",
    "\n",
    "In this section, we are going to go throught the process of creating a custom ML NER model in spaCy. \n",
    "\n",
    "First, let's download the two data files needed for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data files\n",
    "import urllib.request\n",
    "urls = [\n",
    "    'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_HarryPotter_FilmSpells.csv',\n",
    "    'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_HarryPotter_Spells.csv',\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    urllib.request.urlretrieve(url, './data/' + url.rsplit('/', 1)[-1])   \n",
    "print('Sample files ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ba8af",
   "metadata": {},
   "source": [
    "The first file stores the information about the spells in Harry Potter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spells_df = pd.read_csv('./data/NER_HarryPotter_Spells.csv', sep=\";\")\n",
    "spells_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148d7d2",
   "metadata": {},
   "source": [
    "In the second file, we find the characters speaking and their speech. Notice that there is a column storing the spells found in the sentence if there is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a00ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_spells = pd.read_csv('./data/NER_HarryPotter_FilmSpells.csv')\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948edc5",
   "metadata": {},
   "source": [
    "Suppose we would like to create a model that can identify spells in a sentence.\n",
    "\n",
    "In the following, we will first create a NLP model with an entity ruler that identifies spells. This section can be seen as a review of what we have learned about EntityRuler in Wednesday's lesson.\n",
    "\n",
    "Then, we will train a NLP model using word2vec technique that identifies spells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51643760",
   "metadata": {},
   "source": [
    "## Create an NLP model with an EntityRuler to identify the spells\n",
    "\n",
    "Before we create a new EntityRuler, we will do some preprocessing of the data to get the patterns that we will add to the EntityRuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b30d9",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN cells with an empty string\n",
    "spells_df['Incantation'] = spells_df['Incantation'].fillna(\"\")\n",
    "\n",
    "# Get all spells\n",
    "spells = spells_df['Incantation'].unique().tolist() # Put all strs in the 'Incantation' column in a list\n",
    "spells = [spell for spell in spells if spell != ''] # Get all non-empty strs from the list, i.e. all the spells\n",
    "\n",
    "# Take a look at the spells\n",
    "spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067334d",
   "metadata": {},
   "source": [
    "### Creating the patterns to be added to the EntityRuler\n",
    "\n",
    "Recall from Wednesday's lesson that the patterns we add to an EntityRuler look like the following.\n",
    "\n",
    "`patterns = [{\"label\": \"GPE\", \"pattern\": \"Aars\"}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95345f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "for spell in spells:  \n",
    "    spell_dict = {\"label\": \"SPELL\", \"pattern\": spell}\n",
    "    patterns.append(spell_dict)\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865668ad",
   "metadata": {},
   "source": [
    "Now that we have the patterns ready, we can add them to an EntityRuler and add the ruler as a new pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EntityRuler and add the patterns to the ruler\n",
    "entruler_nlp = spacy.blank('en') # Create a blank English model\n",
    "ruler = entruler_nlp.add_pipe(\"entity_ruler\") \n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e938c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Ron Weasley: Wingardium Leviosa! Hermione Granger: You're saying it wrong. \n",
    "It's Wing-gar-dium Levi-o-sa, make the 'gar' nice and long. \n",
    "Ron Weasley: You do it, then, if you're so clever\"\"\"\n",
    "doc = entruler_nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print('EntRulerModel', ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c69b3",
   "metadata": {},
   "source": [
    "## Train a NLP model using ML to identify the spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb6c0b",
   "metadata": {},
   "source": [
    "We have basically hard written all spells in the EntityRuler we add to the entruler model. Therefore, we could just use this model to generate labeled data as our training data and validation data.\n",
    "\n",
    "The format of the training data will look like the following. It is a list of tuples. In each tuple, the first element is the text string containing spells and the second element is a dictionary. The key of the dictionary is 'entities'. The value is a list of lists. In each list, we find the starting index, ending index and the label of the spell(s) found in the text string. \n",
    "\n",
    "`[\n",
    "('Oculus Reparo.', {'entities': [[0, 13, 'SPELL']]}),\n",
    "('Alohomora', {'entities': [[0, 9, 'SPELL']]})\n",
    "]`\n",
    "\n",
    "The text strings we use for the training are from the 'Sentence' column of the film_spells dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the film_spells df\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # for sentence tokenization\n",
    "def generate_labeled_data(text): # the input will be a list of strings\n",
    "    labeled_data = []\n",
    "    for ele in text:\n",
    "        sentences = nltk.sent_tokenize(ele)\n",
    "        for sentence in sentences:\n",
    "            doc = entruler_nlp(sentence) # create a doc object\n",
    "            if doc.ents != (): # if there is at least one entity identified\n",
    "                # for each entity that has been identified, generate a list containing\n",
    "                # the starting index, the ending index and the label for the entity\n",
    "                # these lists are put in a big list which is assigned to the variable ls_ents\n",
    "                ls_ents = [[ent.start_char, ent.end_char, ent.label_] for ent in doc.ents] \n",
    "                entry = (sentence,{\"entities\": ls_ents})\n",
    "                labeled_data.append(entry)\n",
    "    return labeled_data\n",
    "\n",
    "# Assign the result from the function to a new variable\n",
    "training_validation_data = generate_labeled_data(film_spells['Sentence'].tolist())\n",
    "\n",
    "# Take a look at the labeled data\n",
    "training_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6de3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a8751",
   "metadata": {},
   "source": [
    "spaCy 3 requires that our data be stored in the proprietary `.spacy` format. To do that we need to use the `DocBin` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin \n",
    "\n",
    "db = DocBin() \n",
    "\n",
    "for text, annot in training_validation_data[:19*2]: # Get the first 38 tuples as the training data\n",
    "    doc = entruler_nlp(text) # create a doc object\n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./train_spells.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, annot in training_validation_data[19*2:]: # Get the rest tuples as the training data\n",
    "    doc = entruler_nlp(text) \n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./valid_spells.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa73251",
   "metadata": {},
   "source": [
    "Now we can finally start training our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy init config --lang en --pipeline ner config.cfg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy train config.cfg --output ./output/spells-model/ --paths.train ./train_spells.spacy --paths.dev ./valid_spells.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0e1b6",
   "metadata": {},
   "source": [
    "Now let's finally run our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44239291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = spacy.load('./output/spells-model/model-best')\n",
    "model_best.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926983c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"53. Imperio - Makes target obey every command But only for really, really funny pranks. 52. Piertotum Locomotor - Animates statues On one hand, this is awesome. On the other, someone would use this to scare me.\n",
    "\n",
    "51. Aparecium - Make invisible ink appear\n",
    "\n",
    "Your notes will be so much cooler.\n",
    "\n",
    "50. Defodio - Carves through stone and steel\n",
    "\n",
    "Sometimes you need to get the eff out of there.\n",
    "\n",
    "49. Descendo - Moves objects downward\n",
    "\n",
    "You'll never have to get a chair to reach for stuff again.\n",
    "\n",
    "48. Specialis Revelio - Reveals hidden magical properties in an object\n",
    "\n",
    "I want to know what I'm eating and if it's magical.\n",
    "\n",
    "47. Meteolojinx Recanto - Ends effects of weather spells\n",
    "\n",
    "Otherwise, someone could make it sleet in your bedroom forever.\n",
    "\n",
    "46. Cave Inimicum/Protego Totalum - Strengthens an area's defenses\n",
    "\n",
    "Helpful, but why are people trying to break into your campsite?\n",
    "\n",
    "45. Impedimenta - Freezes someone advancing toward you\n",
    "\n",
    "\"Stop running at me! But also, why are you running at me?\"\n",
    "\n",
    "44. Obscuro - Blindfolds target\n",
    "\n",
    "Finally, we don't have to rely on \"No peeking.\"\n",
    "\n",
    "43. Reducto - Explodes object\n",
    "\n",
    "The \"raddest\" of all spells.\n",
    "\n",
    "42. Anapneo - Clears someone's airway\n",
    "\n",
    "This could save a life, but hopefully you won't need it.\n",
    "\n",
    "41. Locomotor Mortis - Leg-lock curse\n",
    "\n",
    "Good for footraces and Southwest Airlines flights.\n",
    "\n",
    "40. Geminio - Creates temporary, worthless duplicate of any object\n",
    "\n",
    "You could finally live your dream of lying on a bed of marshmallows, and you'd only need one to start.\n",
    "\n",
    "39. Aguamenti - Shoot water from wand\n",
    "\n",
    "No need to replace that fire extinguisher you never bought.\n",
    "\n",
    "38. Avada Kedavra - The Killing Curse\n",
    "\n",
    "One word: bugs.\n",
    "\n",
    "37. Repelo Muggletum - Repels Muggles\n",
    "\n",
    "Sounds elitist, but seriously, Muggles ruin everything. Take it from me, a Muggle.\n",
    "\n",
    "36. Stupefy - Stuns target\n",
    "\n",
    "Since this is every other word of the \"Deathly Hallows\" script, I think it's pretty useful.\"\"\"\n",
    "\n",
    "# Create a doc object out of the text string using the trained model\n",
    "doc = model_best(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e12b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc object out of the text string using the EntityRuler model\n",
    "doc = entruler_nlp(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e49ed",
   "metadata": {},
   "source": [
    "It seems in this example our EntityRuler model performs better than our trained model. Why do we think that is?\n",
    "\n",
    "Part of the reason we aren't getting better results is something that Ines Montani describes in this Stack Overflow answer https://stackoverflow.com/questions/50580262/how-to-use-spacy-to-create-a-new-entity-and-learn-only-from-keyword-list/50603247#50603247\n",
    "\n",
    "\"The advantage of training the named entity recognizer to detect SPECIES in your text is that the model won't only be able to recognise your examples, but also generalise and recognise other species in context. If you only want to find a fixed set of terms and not more, a simpler, rule-based approach might work better for you. You can find examples and details of this here.\n",
    "\n",
    "If you do want the model to generalise and recognise your entity type in context, you also have to show it examples of the entities in context. That's currently the problem with your training examples: you're only showing the model single words, not sentences containing the words. To get good results, the data you're training the model with needs to be as close as possible to the data you later want to analyse.\n",
    "\n",
    "While there are other approaches for training models without or with fewer labelled examples, the most straightforward strategy for collecting training data to train your spaCy model is to... label training data. However, there are some tricks you can use to make this less painful:\n",
    "\n",
    "Start with a list of species and use the Matcher or PhraseMatcher to find them in your documents. For each match, you'll get a Span object, so you can extract the start and end position of the span in the text. This easily lets you create a bunch of examples automatically. You can find some more details on this here.\n",
    "\n",
    "Use word vectors to find more similar terms to the entities you're looking for, so you get more examples you can search for in your text using the above approach. I'm not sure how spaCy's vector models will do for your species, since the terms are quite specific. So if you have a large corpus of raw text containing species, you might have to train your own vectors.\n",
    "\n",
    "Use a labelling or data annotation tool. There are open-source solutions like Brat, or, once you're getting more serious about annotation and training, you might also want to check out our annotation tool Prodigy, which is a modern commercial solution that integrates seamlessly with spaCy (Disclaimer: I'm one of the spaCy maintainers).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01473f",
   "metadata": {},
   "source": [
    "# References\n",
    "McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
