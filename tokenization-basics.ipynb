{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50463b40",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f72056",
   "metadata": {},
   "source": [
    "\n",
    "# Tokenization Basics\n",
    "\n",
    "**Description:**\n",
    "This notebook focuses on the basic concepts surrounding tokenization. It includes material on the following concepts:\n",
    "\n",
    "* Word segmentation\n",
    "* n-grams\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Tokenizers\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 60-90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics 1](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* [Working with Dataset Files](./working-with-dataset-files.ipynb)\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:**\n",
    "* re\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Scan documents\n",
    "2. OCR files\n",
    "3. Clean up texts\n",
    "4. **Tokenize text files** (this notebook)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7737942",
   "metadata": {},
   "source": [
    "## What is a word?\n",
    "\n",
    "The concept of a word makes intuitive sense in everyday language, but it starts to break down significantly when we begin trying to formalize it for analysis with computer programs. Linguists have spent decades creating formal rules for breaking down texts into smaller parts for analysis, dealing in great detail with the normally unspoken rules of grammar. In this lesson, we consider what a word is and consider how we could write a program for collecting the words within a text.\n",
    "\n",
    "Let's take a look at an example sentence:\n",
    "\n",
    "> Now that summer's here, we're going to visit the beach at Lake Michigan and eat ice cream.\n",
    "\n",
    "How many words are in this sentence? We could start by simply looking at words that are separated by spaces. \n",
    "\n",
    "> Now, that, summer's, here, we're, going, to, visit, the, beach, at, Lake, Michigan, and, eat, ice, cream.\n",
    "\n",
    "That would give us 17 words. But we could ask a few questions about this count. For example, is 'Lake Michigan' one word or two words? Certainly, lake and Michigan have their own individual meanings, but Lake Michigan certainly has a different meaning from either of those words individually. Similarly, what about 'ice cream'?\n",
    "\n",
    "What about contractions? Is 'we're' a single word or two words: 'we' and 'are'? If our goal is to count how many times a given word occurs in the sentence, does 'we' occur in the sentence? Does the word 'summer' occur in our sentence?\n",
    "\n",
    "Verb conjugations pose yet another problem. Should the word 'going' be counted separately from 'go'. What about 'went'? From a computational linguistics perspective, we could 'stem' words, simply lopping off the 'ing' from 'going' to get 'go'. But that would poses some serious programming challenges for words like 'running' where the base form is 'run' instead of 'runn'. And we might run into issues with words 'sing' or 'singing' that should not have 'ing' removed in the former case but once in the later case. How could we distinguish between words that are conjugated 'sings' and words that are plural 'wings'. Sometimes an -s ending is plural (fens) and other times it is not (lens).\n",
    "\n",
    "Tokenization, or segmenting a text into word chunks, is the first part of a Natural Language Processing pipeline, and it can have significant effects on the results of your analysis. We will focus on tokenizing a text to create a bag of words model. A bag of words approach will help us break down our text into one-, two-, and three-word constructions. The general name for these is n-grams.\n",
    "\n",
    "An n-gram is a sequence of n items from a given sample of text or speech. Most often, this refers to a sequence of words, but it can also be used to analyze text at the level of syllables, letters, or phonemes. N-grams are often described by their length. For example, word n-grams might include:\n",
    "\n",
    "* stock (a 1-gram, or unigram)\n",
    "* vegetable stock (a 2-gram, or bigram)\n",
    "* homemade vegetable stock (a 3-gram, or trigram)\n",
    "\n",
    "A text analysis approach that looks only at unigrams at the word level will not be able to differentiate between the \"stock\" in \"stock market\" and \"chicken stock.\" One of the most popular examples of text analysis with n-grams is the [Google N-Gram Viewer](https://books.google.com/ngrams).\n",
    "\n",
    "Before we examine several Python tokenizers, it is important to understand more about how Python strings work. Tokenization is a process that segments strings into smaller chunks and part of getting good results is understanding the decisions that tokenizers make. A deeper understanding of strings will help, not just with tokenization, but the whole text analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b5b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
