{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fac83f3-4536-4584-870a-6bcfec93ea68",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "This notebook is created by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef364-f331-462d-96f1-4cd736b4dcec",
   "metadata": {},
   "source": [
    "# Pandas 3 \n",
    "\n",
    "**Description:** This notebook describes how to:\n",
    "* Build a dataset from Constellate\n",
    "* Make a dataframe from the dataset\n",
    "* Summarize data in a dataframe\n",
    "* Group and aggregate data\n",
    "* Make pivot tables in Pandas\n",
    "\n",
    "This is the third notebook in a series on learning to use Pandas. \n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Knowledge Required:** \n",
    "* [Pandas 1](./pandas-1.ipynb)\n",
    "* [Pandas 2](./pandas-2.ipynb)\n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* [Python Intermediate 1](./python-intermediate-1.ipynb)\n",
    "* [Python Intermediate 2](./python-intermediate-2.ipynb)\n",
    "* [Python Intermediate 4](./python-intermediate-4.ipynb)\n",
    "\n",
    "**Completion Time:** 60 minutes\n",
    "\n",
    "**Data Format:** JSONL \n",
    "\n",
    "**Libraries Used:** Pandas\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af26b1",
   "metadata": {},
   "source": [
    "# Build a dataset from Constellate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46d211",
   "metadata": {},
   "source": [
    "The dataset we are going to use for today's lesson is the documents from JSTOR or Portico with the key word \"machine learning\" or \"artificial intelligence\" about Arts, History, Philosophy, Religion limited to document type(s) article, chapter, book from 2011 - 2020. There are 12,286 documents in this dataset in total. I have built this dataset before class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ecd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import constellate\n",
    "import constellate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887ff28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a variable `dataset_id` to hold our dataset ID\n",
    "dataset_id = 'd6232206-93bf-f6b8-9ad2-b2add01cf231'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74ccac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use constellate.download() to download the dataset\n",
    "# in the Constellate Document Format (jsonl) and give the file a name\n",
    "dataset_file = constellate.download(dataset_id, 'jsonl', 'ML_AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b417ad",
   "metadata": {},
   "source": [
    "See the Constellate Client documentation at: https://constellate.org/docs/constellate-client for the different downloading options. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58119",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "After we download the dataset, we can use the `dataset_reader()` method to read in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6a5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the .dataset_reader() method to read in the documents\n",
    "docs = constellate.dataset_reader(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c2a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the type of docs\n",
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f883602",
   "metadata": {},
   "source": [
    "Recall from [Python Intermediate 5](./python-intermediate-5.ipynb) that the difference between a list and a generator is that the latter yields only one element at a time. As a result, generators are more memory-efficient than lists. \n",
    "\n",
    "To return the elements in a generator one by one, we use the `next()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257afb6d-ecf5-466a-9ab5-97ceefcac854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Take a look at the first element of the generator docs\n",
    "doc1 = next(docs)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82ecf9",
   "metadata": {},
   "source": [
    "We can see that the document is loaded as a Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df0592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all keys from the dict\n",
    "doc1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628fb7f-2378-4518-b77b-e54b828f75ee",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Now you know that a Constellate dataset of documents with metadata, ngrams and possibly full text is read into a generator of dictionaries. Can you follow the prompts below to create a dataframe out of a Constellate dataset? Use what you have learned from [Python Basics](./python-basics-1.ipynb), [Pandas 1](./pandas-1.ipynb) and [Pandas 2](./pandas-2.ipynb) to do this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33160-fbe3-49b2-9a39-aa5c6a559f21",
   "metadata": {},
   "source": [
    "Step 1. Complete the following code cell to download a Constellate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6a08a-fc58-4535-9b22-78864f8fa05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from a Constellate dataset\n",
    "dataset_id = 'f6ae29d4-3a70-36ee-d601-20a8c0311273'\n",
    "\n",
    "# Use constellate.get_dataset() to download the dataset(sampled to 1500 documents)\n",
    "path = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c971e8-6006-4c94-a10e-4739af607c7b",
   "metadata": {},
   "source": [
    "Step 2. In the next code cell, read in the data from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba996b-ffae-4852-9952-eb1eb62fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use constellate.dataset_reader() to read in the data\n",
    "docs = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc4733-4f4c-4f97-9650-ff3aa21cdbc7",
   "metadata": {},
   "source": [
    "Step 3. Get the keys from the first document in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ecb49-47ff-4d1d-9843-b8cf34cdfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first document from the dataset\n",
    "doc1 = next(data)\n",
    "\n",
    "# Get the keys from the first doc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cd38f-5381-4ed5-aec2-439c74f67a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the keys that are of interest to you and\n",
    "# put them in a list called keys_of_interest\n",
    "keys_of_interest = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5aadd8-7709-41c0-ba41-788a4a626446",
   "metadata": {},
   "source": [
    "Step 4. In the next code cell, create a dataframe storing the data of interest to you. The headers of the dataframe are the keys of interest you have just selected. Each row of the dataframe contains the relevant data from one document of the dataset. For example, if you choose 'id' and 'publicationYear' as the keys of interest. Then, the first row of the dataframe will have the id of the first document in the 'id' column and the publication year of the first document in the 'publicationYear' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f6bd0-adf6-4ba6-9ea5-8854a16e8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Get the docs again\n",
    "docs = constellate.dataset_reader(path)\n",
    "\n",
    "# Create a dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac923c8c",
   "metadata": {},
   "source": [
    "## Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514925b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f765b21",
   "metadata": {},
   "source": [
    "Suppose not all data in the documents are of interest to us. Let's select the data we are interested in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c74dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data of interest\n",
    "data_of_interest = ['id', 'title', 'docType', 'publicationYear', 'bigramCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b520a-2458-45f1-a1fb-506eb760bbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the docs again\n",
    "docs = constellate.dataset_reader(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5f56b",
   "metadata": {},
   "source": [
    "From each doc in docs, we want to grab the values corresponding to the keys in the list of `data_of_interest` and create a dataframe from the data. For a quick review of list comprehensions, take a look at [Python Intermediate 1](./python-intermediate-1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6714596",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the data we need for creating a dataframe\n",
    "data = [[doc['id'], doc['title'], doc['docType'], doc['publicationYear'], doc['bigramCount']] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264828c4-7fd1-4d79-b6cc-2d776e12444d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "df = pd.DataFrame(data, columns=['id', 'title', 'docType', 'publicationYear', 'bigramCount'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4436d-cb79-4ac6-9f2f-d6ee43b7e9a2",
   "metadata": {},
   "source": [
    "## Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aba96",
   "metadata": {},
   "source": [
    "We will often need to do some data cleaning and pre-processing after we create a dataframe. What kind of data cleaning and pre-processing you need to do depends on the specific task at hand. Here, I only give some examples. \n",
    "\n",
    "When we look at the 'id' column, we can see that all document ids start with \"ark://27972/\". We can get rid of this prefix and use the rest of the string as the ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f00766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shorten the ids\n",
    "df['id'] = df['id'].apply(lambda r: r.rsplit('/')[-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dc665-6b4d-4639-9a8b-c35c35153eb0",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "The bigramCount column gives the number of occurrences of every bigram string in a document. As you can see, the puntuations do not count as a gram. This is why 'life. But' is seen as a bigram, not a trigram. With this in mind, can you make a new column storing the count of the bigram 'machine learning' and a new column storing the bigram 'artificial in telligence'? You can drop the bigramCount column after making the two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40dfac-dfd2-4eff-b7a4-5c2fa480035b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc7363-5ca8-4805-b5bc-15b69be6d166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c25766-55b3-409a-8cb4-06f088069110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e96453-464f-4a05-ac26-8d18fe0592f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example solution \n",
    "len_ML = len('machine learning')\n",
    "\n",
    "# Define a function to return the count of 'machine learning' and 'artificial intelligence'\n",
    "def count(r):\n",
    "    count_ML = 0\n",
    "    count_AI = 0\n",
    "    for key in r.keys():\n",
    "        key_lower = key.lower()\n",
    "        if len(key)<len_ML:\n",
    "            continue\n",
    "        elif 'machine learning' in key_lower:\n",
    "            count_ML += r[key]\n",
    "        elif 'artificial intelligence' in key_lower:\n",
    "            count_AI += r[key]\n",
    "    return [count_ML, count_AI]\n",
    "\n",
    "# Create a column storing the count of 'machine learning'\n",
    "df[['ML_count', 'AI_count']] = df['bigramCount'].apply(count).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b539524-c3f4-41f8-ad79-9ec8d9003da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the updated df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dde9b",
   "metadata": {},
   "source": [
    "## Group and aggregate data\n",
    "\n",
    "After data cleaning, filtering and preprocessing, the next step is to summarize the data to extract useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345dccf",
   "metadata": {},
   "source": [
    "Pandas makes summarising a dataframe very easy. For example, we can count how many non-null values there are in each column using the `.count()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514212fd-0a20-4693-a123-a0a38526490a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of non-null values in each column\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5d271-1a71-4370-935f-c20a8166cb93",
   "metadata": {},
   "source": [
    "We can also get the max value or the min value of a column using the `.max()` and `.min()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e614-44aa-40a6-942a-bcb6ecc74198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the max value from the year column\n",
    "df['publicationYear'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e90c3-9831-4645-841c-bbab9a67848a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the min value from the year column\n",
    "df['publicationYear'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c345c-9e7b-4d86-b222-f06f29de02b2",
   "metadata": {},
   "source": [
    "You can refer to the Pandas documentation for more methods that you can use to query the data. \n",
    "\n",
    "When you summarize a dataframe, a very useful method is `.describe()`. It can quickly display the statistics for any group of data it is applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d7834-7918-4275-807e-1e5a0cd32fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the .describe() method to explore the year column\n",
    "df['title'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd5c24-aee2-44a8-bffd-a0547914b698",
   "metadata": {},
   "source": [
    "### Groupby()\n",
    "\n",
    "Groupby is a powerful function built into Pandas that you can use to summarize your data. Groupby splits the data into different groups on a variable of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c39249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the data by docType\n",
    "df.groupby('docType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c22429-0eea-4110-8f60-027f3be9fff0",
   "metadata": {},
   "source": [
    "The groupby() method returns a GroupBy object which describes how the rows of the original dataset have been split by the selected variable. You can actually see how the rows of the original dataframe have been grouped using `groups` after applying `groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751abaf1-7595-44e0-b705-4488ba2972ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how the rows have been grouped\n",
    "df.groupby('docType').groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d26d3-1793-4137-ac6f-69b35e90865e",
   "metadata": {},
   "source": [
    "As you can see, a dictionary is returned whose keys are the unique values in docType and whose values are lists of row indexes. Each key corresponds to a list of row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49eb9d-bf70-40c3-b399-dfd271008a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the keys from the groups dictionary\n",
    "df.groupby('docType').groups.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa87067-707f-4fef-bd6c-6b25b7dd7c7d",
   "metadata": {},
   "source": [
    "You can group the data using multiple variables. For example, you may want to group the documents first by their publication year and then by the document type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e538d-51e7-4028-b561-aadc5ad92d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by a composite variable ['publicationYear', 'docType']\n",
    "df.groupby(['publicationYear', 'docType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb55aa-459b-41d8-8a86-876239bc8aea",
   "metadata": {},
   "source": [
    "If you take a look at the groups in the groupby object, you will see that essentially we have a composite key for each group. The first key, for example, is (2011, 'article'). The value associated with this key is a list of indexes, all of which are the rows storing the documents that were published in 2011 and are of the docType 'article'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbad271-7b0d-49b0-ba25-128ab50bd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple variables \n",
    "# Take a look at the composite keys\n",
    "df.groupby(['publicationYear', 'docType']).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5b819-dd08-46df-9378-5b1915d26485",
   "metadata": {},
   "source": [
    "Of course, we don't just stop at grouping data. Grouping data is just a step towards data query. After we apply the `.groupby()` method, we can actually use different Pandas methods to query the data. For example, how do we get the number of documents in each docType by publicationYear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbcd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a series storing the number of documents in each doc type by year\n",
    "df.groupby(['publicationYear', 'docType']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8064b-8813-4b6e-80ed-04282a19ca08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T17:23:44.615027Z",
     "iopub.status.busy": "2023-03-23T17:23:44.614636Z",
     "iopub.status.idle": "2023-03-23T17:23:44.638710Z",
     "shell.execute_reply": "2023-03-23T17:23:44.638324Z",
     "shell.execute_reply.started": "2023-03-23T17:23:44.614994Z"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493b3302-c684-4504-8366-bc435e8e85c8",
   "metadata": {},
   "source": [
    "### Agg() \n",
    "\n",
    "After we group the data in a dataframe, we can apply the `agg()` method to calculate multiple statistics per group in one calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3208ee0-8af7-4aca-afe8-95022f1f7a54",
   "metadata": {},
   "source": [
    "For example, let's say we would like to know the sum of the occurrences of the bigram 'machine learning' in all the documents from each year. To achieve this goal, we can group the data by `publicationYear`, and then aggregate the data by summing the numerical values in the column of `ML_count` for each subgroup.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bed1b-ef1c-4cdc-9209-4cd64e83a856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get how many times 'machine learning' is\n",
    "# mentioned in the docs each year\n",
    "df.groupby('publicationYear').agg({'ML_count':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823bee7-1676-467c-a949-5eac4e5755d2",
   "metadata": {},
   "source": [
    "Of course, you can choose other ways to aggregate the data in each subgroup. For example, suppose you are interested in the biggest frequency with which a document mentions 'artificial intelligence' by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15539a07-4726-49f5-a55c-a3cff7ad78d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the biggest frequency of a document mentioning \n",
    "# 'artificial intelligence' by year\n",
    "df.groupby('publicationYear').agg({'AI_count':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd639145-c660-46fc-b1a9-99d347c5801c",
   "metadata": {},
   "source": [
    "We can specify multiple columns to apply a function to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30901a-1fde-4192-ba9b-b22cdcc0bed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply a single function to selected columns in each subgroup\n",
    "df.groupby('publicationYear').agg({'AI_count':'sum', 'ML_count': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06350d24-1b26-4269-8672-7d02cc79cb61",
   "metadata": {},
   "source": [
    "We can also apply multiple functions to each of the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab56a3-7e0b-48a8-955f-af482dae4386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply multiple functions to selected columns in each subgroup\n",
    "df.groupby('publicationYear').agg({'AI_count':['sum', 'max'], 'ML_count':['max', 'count']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c819d1d-edd4-457c-87b9-d9acd4e96acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T22:41:09.904787Z",
     "iopub.status.busy": "2023-03-23T22:41:09.904101Z",
     "iopub.status.idle": "2023-03-23T22:41:09.926725Z",
     "shell.execute_reply": "2023-03-23T22:41:09.926110Z",
     "shell.execute_reply.started": "2023-03-23T22:41:09.904723Z"
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Take the following dataframe containing the information on the Covid19 cases in the state of Massachusetts. Can you work with the dataframe to find out which month of which year has the most positive new cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbda0c5-3d81-4cd7-94f2-8408802c6081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the data of covid19 cases in MA and \n",
    "# create a dataframe \n",
    "import urllib\n",
    "url = 'https://www.mass.gov/doc/covid-19-raw-data-march-9-2023/download'\n",
    "urllib.request.urlretrieve(url, './data/covid_MA')\n",
    "covid_ma = pd.read_excel(url, 'Cases (Report Date)')\n",
    "covid_ma['Date'] = covid_ma['Date'].astype(str)\n",
    "\n",
    "# Take a look at the dataframe\n",
    "covid_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541123d-3051-454e-9d25-39abef878a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a332c2-bdbd-4959-8e6c-9c2037af0592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T23:45:35.059972Z",
     "iopub.status.busy": "2023-03-23T23:45:35.059285Z",
     "iopub.status.idle": "2023-03-23T23:45:35.081367Z",
     "shell.execute_reply": "2023-03-23T23:45:35.080850Z",
     "shell.execute_reply.started": "2023-03-23T23:45:35.059908Z"
    },
    "tags": []
   },
   "source": [
    "## Make pivot tables in Pandas\n",
    "\n",
    "Pandas has a `.pivot_table()` method we can use to summarize data. It takes a dataframe as argument and has parameters specifying the shape of the pivot table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b8775-fe8d-4fc0-8384-82c1f8c1585d",
   "metadata": {},
   "source": [
    "In the previous section, we have used the `.groupby()` and `agg()` methods to summarize data. For example, we grouped the documents in the dataframe df by their year of publication and calculated the sum of the mentions of the bigram 'artificial intelligence'. We can do the same thing using the `.pivot_table` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3989f0a-095b-4fa5-9643-e69d799a18cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of \n",
    "# the mentions of 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values='AI_count',\n",
    "                      aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e2f88-b916-4e19-8bbf-559cd40aea47",
   "metadata": {},
   "source": [
    "Again, when aggregating the data, you can apply a single function to multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec95c4-8f78-4ac7-92c1-dcd980e6e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of \n",
    "# the mentions of 'machine learning' and 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values=['AI_count', 'ML_count'],\n",
    "                      aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895b079-d235-4106-b0b2-138835662439",
   "metadata": {},
   "source": [
    "You can also apply multiple functions to a single column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd494ac2-f741-4225-a298-6b6cc0b6745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum and the max value of\n",
    "# the mentions of 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values='AI_count',\n",
    "                      aggfunc=['sum', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc07c4a-65a4-4460-b439-8ffc83fa8bea",
   "metadata": {},
   "source": [
    "Or, you can apply different functions to different columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14007f22-f2fe-4a51-816b-a783a646052d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of\n",
    "# the mentions of 'artificial intelligence' by year\n",
    "# and the max value of the mentions of 'machine learning' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values=['AI_count', 'ML_count'],\n",
    "                      aggfunc={'AI_count':'sum', 'ML_count':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc76b40-f319-4099-8ea8-e8c7ebcfd3ec",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Get the dataframe stored in the variable `covid_ma`. Can you make a pivot table showing the sum of the positive cases from 2020 - 2023 in that table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53047da1-15ce-4c9a-8eab-45ec10055e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73e1ef9-49e3-4a93-b6a9-db4878bbb2d0",
   "metadata": {},
   "source": [
    "## A teaser for the Data Visualization class\n",
    "\n",
    "We have learned how to create a dataset from Constellate, how to preprocess the data and how to summarize the data. With the information we get from summarizing the data, we can go ahead and plot it!\n",
    "\n",
    "For example, let's plot the number of docs that mentioned 'artificial intelligence' or 'machine learning' from 2011 - 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9cc31-f218-42c1-aa24-2d89c88bb60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the dataframe for plotting\n",
    "df.groupby('publicationYear').size().plot(kind='bar', ylabel='num_doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9adb7-a4bd-4840-9b87-9bac380767c7",
   "metadata": {},
   "source": [
    "___\n",
    "# Lesson Complete\n",
    "Congratulations! You've completed the *Pandas* series. \n",
    "\n",
    "Considering the amount of material in *Pandas 1-3* there's a good chance you won't retain it all. That's okay. Programmers often need to look up things to accomplish a task they haven't done in a while, particularly if it is in a language they don't often use. When you're working on a project, you can always come back to these lessons as reference materials. In other words, you've learned an incredible amount, so don't be surprised if it doesn't all stick at first.\n",
    "\n",
    "If you want to help yourself retain what you've learned, the best way is to start putting it into practice. Try your hand at creating some small Pandas projects and recognize that the things you've learned here will cement with time and practice. When you do forget a particular thing&mdash;as we all do&mdash;a quick web search often turns up some useful examples.\n",
    "\n",
    "## Start a Text Analysis Lesson:\n",
    "* [Exploring Metadata](./exploring-metadata.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2b8c3",
   "metadata": {},
   "source": [
    "## Solutions to exercises\n",
    "\n",
    "Here are the solutions to some of the exercises in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out which month of which year has the most positive cases of covid19 in MA\n",
    "\n",
    "# Download the file\n",
    "import urllib\n",
    "url = 'https://www.mass.gov/doc/covid-19-raw-data-march-9-2023/download'\n",
    "urllib.request.urlretrieve(url, './data/covid_MA')\n",
    "\n",
    "# Create a dataframe from the relevant sheet\n",
    "covid_ma = pd.read_excel(url, 'Cases (Report Date)')\n",
    "covid_ma['Date'] = covid_ma['Date'].astype(str) # process the date column\n",
    "\n",
    "# Extract the year and month from the date column\n",
    "covid_ma['group_var'] = covid_ma['Date'].apply(lambda r: r.rsplit('-', 1)[0])\n",
    "\n",
    "# Use the new column as a grouping variable and divide the data into subgroups\n",
    "# Aggregate the data using 'sum' function and sort the results in descending order\n",
    "covid_ma.groupby('group_var').agg({'Positive New':'sum'}).sort_values('Positive New', ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c4c99-6865-463f-8814-345a7c8c4ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Make a pivot table showing the sum of positive covid19 cases by year in covid_ma\n",
    "\n",
    "covid_ma['Year'] = covid_ma['Date'].str.slice(0,4)\n",
    "covid_ma.pivot_table(index='Year',\n",
    "                    values='Positive New',\n",
    "                    aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724f975",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
